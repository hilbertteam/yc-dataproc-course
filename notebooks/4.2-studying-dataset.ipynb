{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fde7b7-0143-4e92-b6dc-94137074fd2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%delete_livy_session --cluster dataproc-course --id lesson-4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd69686-4bc6-4631-8ff0-b1a77a757f63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%create_livy_session --cluster dataproc-course --id lesson-4.2 --conf spark.cores.max=1 --conf spark.executor.memory=2g --conf spark.driver.memory=2g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60fafe3-d288-4c92-ad88-be249298c10d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!spark --cluster dataproc-course --session lesson-4.2\n",
    "\n",
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv('s3a://yc-dataproc-tasks/data/transaction_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10135389-6539-4720-92ee-52c2aa4e169c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2b846e-578f-4105-8e10-ee77cc299105",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   ===============    Обзор полученных данных   ==============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b46fcd7-7325-49d8-9e1d-b2b022847a4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!spark --cluster dataproc-course --session lesson-4.2 \n",
    "\n",
    "# Получаем массив n строк\n",
    "for i in df.take(2):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32795d5d-82e2-4ea7-bddb-7fcae0aa02c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!spark --cluster dataproc-course --session lesson-4.2 \n",
    "\n",
    "df = spark.read.option(\"header\", True).csv('s3a://yc-dataproc-tasks/data/transaction_data.csv')\n",
    "\n",
    "# Показываем 10 строк в обрезанном состоянии\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c6fd1f-2643-479f-a1da-f5c8f4b380a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!spark --cluster dataproc-course --session lesson-4.2 \n",
    "\n",
    "# Выбирает из датафрейма n верхних строк\n",
    "df.limit(5).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3753197b-8020-49ce-a594-e3b5b3e8d811",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!spark --cluster dataproc-course --session lesson-4.2 \n",
    "\n",
    "# Показываем 10 строк в обрезанном состоянии\n",
    "df.limit(10).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5463f5-ff1b-4fa3-b601-cdb4203489a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b568b069-ac0e-44e8-b475-9177476209c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   ===============    Изучение структуры данных   ==============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a23c52b-969b-4463-9380-34e12c86aa83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!spark --cluster dataproc-course --session lesson-4.2 \n",
    "\n",
    "# Выбирает из датафрейма n верхних строк\n",
    "for column_name in df.columns:\n",
    "    print(f\"Column '{column_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b7d19c-6345-4818-b296-4c5867c22ec5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!spark --cluster dataproc-course --session lesson-4.2 \n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType\n",
    "\n",
    "print(\"Выводим список атрибутов с помощью метода dtypes\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nВыводим список атрибутов с помощью метода schema\")\n",
    "print(df.schema)\n",
    "\n",
    "df_fields = df.schema.fields\n",
    "\n",
    "print(\"\\nПреобразовываем все атрибуты с типом IntegerType в DoubleType\")\n",
    "new_schema = StructType([StructField(field.name, DoubleType(), field.nullable) if field.dataType == IntegerType() else field for field in df_fields])\n",
    "\n",
    "print(new_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73068e48-97ee-4000-a151-a3bda43732d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!spark --cluster dataproc-course --session lesson-4.2 \n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9277066-af82-4ed9-8db4-4dfcb3d31833",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!spark --cluster dataproc-course --session lesson-4.2 \n",
    "\n",
    "df = spark\\\n",
    "        .read\\\n",
    "        .option(\"inferSchema\", True)\\\n",
    "        .option(\"header\", True)\\\n",
    "        .csv('s3a://yc-dataproc-tasks/data/transaction_data.csv')\n",
    "\n",
    "# Выбирает из датафрейма n верхних строк\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05406c4-98f4-43e0-a783-46f78d04de54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!spark --cluster dataproc-course --session lesson-4.2 \n",
    "from pyspark.sql.functions import col,lit\n",
    "from datetime import datetime\n",
    "\n",
    "current_datetime = datetime.now()\n",
    "\n",
    "df.withColumn('processed_dttm', lit(current_datetime)).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803d5fd8-3a38-4a79-83a8-b0104c377884",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!spark --cluster dataproc-course --session lesson-4.2\n",
    "\n",
    "df.select(\"UserId\", \"Country\").show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d854bc-c895-4f5a-82ff-55b3b2279e27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!spark --cluster dataproc-course --session lesson-4.2\n",
    "df.select(\"Country\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083db375-2b09-4315-a913-acc017d2e9c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!spark --cluster dataproc-course --session lesson-4.2\n",
    "\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([ \\\n",
    "      StructField(\"UserId\", IntegerType()), \\\n",
    "      StructField(\"TransactionId\", IntegerType()), \\\n",
    "      StructField(\"TransactionTime\", StringType()), \\\n",
    "      StructField(\"ItemCode\", IntegerType()), \\\n",
    "      StructField(\"ItemDescription\", StringType()), \\\n",
    "      StructField(\"NumberOfItemsPurchased\", StringType()), \\\n",
    "      StructField(\"CostPerItem\", StringType()), \\\n",
    "      StructField(\"Country\", StringType()) \\\n",
    "    ])\n",
    "        \n",
    "df = spark\\\n",
    "        .read\\\n",
    "        .option(\"header\", True)\\\n",
    "        .schema(schema)\\\n",
    "        .csv('s3a://yc-dataproc-tasks/data/transaction_data.csv')\n",
    "    \n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b0aa67-01cf-4da5-839b-08c7b1bd9936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c07a112-9566-4d8c-94c3-ce182a157a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   ===============    Манипуляция со столбцами   ==============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8120dfe-5337-4660-8030-8ff2200dace5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!spark --cluster dataproc-course --session lesson-4.2\n",
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "# Выбор столбцов\n",
    "print(\"Передаем на вход массив атрибутов\")\n",
    "df.select(df.columns).show(2, False)\n",
    "\n",
    "print(\"Атрибуты по именам\")\n",
    "df.select(\"UserId\", \"Country\").show(2, False)\n",
    "\n",
    "print(\"Используем объект Column\")\n",
    "df.select((col(\"UserId\") * 100).alias(\"UserId\"), col(\"Country\"), lit(\"hello World\").alias(\"greetings\")).show(2, False)\n",
    "\n",
    "print(\"Обращение к атрибуту\")\n",
    "df.select(df.ItemCode, (df.TransactionId - 1000000).alias(\"newTransaction\")).show(2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9cbac8-6ba5-4efd-ae0c-382f160e287c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!spark --cluster dataproc-course --session lesson-4.2\n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "# Выбор столбцов\n",
    "df\\\n",
    "    .dropDuplicates([\"Country\"])\\\n",
    "    .selectExpr(\"UserId\",\\\n",
    "                \"date_format(to_timestamp(TransactionTime, 'E MMM dd HH:mm:ss z yyyy'), 'HH:mm:ss.SSS yyyy/MMM/dd') as timestamp\",\\\n",
    "                \"concat(Country, ' test') as cntr\",\\\n",
    "                \"'hello world!' as greeting\"\n",
    "               )\\\n",
    "    .show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada5a729-8071-414a-a64a-935abb3f07ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!spark --cluster dataproc-course --session lesson-4.2\n",
    "# Добавление и изменение атрибутов\n",
    "from pyspark.sql.functions import when, lower, regexp_replace, length, expr\n",
    "\n",
    "df\\\n",
    "    .select(\"ItemCode\", \"Country\")\\\n",
    "    .dropDuplicates([\"Country\"])\\\n",
    "    .withColumn(\"spaceCount\", when(length(regexp_replace(col(\"Country\"), \"\\S\", \"\")) == 1, \"Single Space\")\n",
    "                .when(length(regexp_replace(col(\"Country\"), \"\\S\", \"\")) > 1, \"Multiple Space\")\n",
    "                .otherwise(\"No spaces\")\n",
    "               )\\\n",
    "    .withColumn(\"ItemCode\", expr(\"case when ItemCode > 450000 then ItemCode + 500000 else round(sqrt(ItemCode), 2) end\"))\\\n",
    "    .show(30, truncate = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3ae831-c5d6-41b2-b195-d46f4bd5b6b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d61e97-8b0a-4b98-acf2-01cbf8d23e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   ===============    Манипуляция со строками датафрейма   ==============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8881dbc9-735e-468b-8a22-a2eef84f4bca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!spark --cluster dataproc-course --session lesson-4.2\n",
    "# Сортировка\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_without_duplicates = df.select(\"UserId\", \"TransactionId\", \"ItemCode\").dropDuplicates([\"UserId\"])\n",
    "\n",
    "df_without_duplicates.sort(col(\"UserId\")).show(10)\n",
    "print(\"Сортировка с использованием sort\")\n",
    "df_without_duplicates.sort(col(\"UserId\").desc(), col(\"ItemCode\").asc()).show(10)\n",
    "print(\"Сортировка с использованием orderBy\")\n",
    "df_without_duplicates.orderBy(col(\"UserId\").desc(), col(\"ItemCode\").asc()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290272bd-40d7-4a86-8e9a-2a3cee485e4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!spark --cluster dataproc-course --session lesson-4.2\n",
    "# Фильтрация\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "item_df = df.select(\"ItemCode\", \"ItemDescription\", \"CostPerItem\")\n",
    "\n",
    "print(\"Фильтрация с использованием where\")\n",
    "item_df.where((col(\"CostPerItem\") > 2.0) & (col(\"CostPerItem\") < 3.0)).show(truncate = False)\n",
    "print(\"Фильтрация с использованием filter\")\n",
    "item_df.filter((col(\"CostPerItem\") > 2.0) & (col(\"ItemDescription\").like(\"LUNCH%\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5298ae1-0d67-48d6-8461-1540c0e9eefe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!spark --cluster dataproc-course --session lesson-4.2\n",
    "\n",
    "# Получение уникальных строк\n",
    "user_df = df.select(\"UserId\", \"TransactionId\", \"Country\")\n",
    "\n",
    "print(\"Убираем полные дубли (по всем строкам)\")\n",
    "user_df.distinct().sort(\"UserId\", \"Country\").show(10)\n",
    "print(\"Убираем дубли только для атрибута UserId\")\n",
    "user_df.dropDuplicates([\"UserId\"]).sort(\"UserId\").show(10)\n",
    "print(\"Убираем дубли для атрибутов UserId и Country\")\n",
    "user_df.dropDuplicates([\"UserId\", \"Country\"]).sort(\"UserId\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e3d0a3-010f-4123-a67c-d8747cd43bdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2f0957-775c-4c3f-acd5-33229ee39d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   ===============    Агрегаты и обогащение   ==============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15c9dc9-9568-47e0-a527-f14708539a3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!spark --cluster dataproc-course --session lesson-4.2 \n",
    "from pyspark.sql.functions import avg, count, min, max, sum\n",
    "\n",
    "# Получаем среднюю стоимость для каждого товара и количество купленных товаров\n",
    "print(\"Получаем среднюю стоимость товара и сумму, сколько раз товар был куплен\")\n",
    "df.select(\"ItemCode\", \"CostPerItem\", \"NumberOfItemsPurchased\")\\\n",
    "    .groupBy(\"ItemCode\")\\\n",
    "    .agg(avg(\"CostPerItem\").alias(\"AverageCost\"), sum(\"NumberOfItemsPurchased\").alias(\"CountOfPurchasedItems\"))\\\n",
    "    .show(10, False)\n",
    "\n",
    "# Получаем количество товаров, которое приобрел каждый пользователь\n",
    "print(\"Получаем данные о том, сколько товаров приобрел каждый пользователь\")\n",
    "df.select(\"UserId\", \"ItemCode\").distinct().groupBy(\"UserId\").agg(count(\"*\").alias(\"ItemCount\")).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7b3c42-5fec-4613-ae63-32aa92daef59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!spark --cluster dataproc-course --session lesson-4.2 \n",
    "from pyspark.sql.functions import col, lit\n",
    "from datetime import datetime\n",
    "\n",
    "# Определяем текущую временную метку\n",
    "current_datetime = datetime.now()\n",
    "\n",
    "# Обогащаем датафрейм новым полем - информация о времени обработки датафрейма\n",
    "df.select(\"UserId\", \"TransactionId\", \"ItemCode\").withColumn('processed_dttm', lit(current_datetime)).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5512f58-dbd0-4ab9-a013-caa4d1237876",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3888e289-3258-4a6e-90cd-88ed26926dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   ===============    Функции   ==============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf53168-71bc-4715-bd98-4cfc305a7b47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!spark --cluster dataproc-course --session lesson-4.2 \n",
    "from pyspark.sql.functions import col, lower, upper, substring, split, trim, regexp_replace, length\n",
    "\n",
    "# Функции для работы со строками\n",
    "df\\\n",
    "    .dropDuplicates([\"Country\"])\\\n",
    "    .limit(10)\\\n",
    "    .select(\n",
    "        \"Country\",\\\n",
    "        lower(\"Country\").alias(\"lower\"),\\\n",
    "        upper(\"Country\").alias(\"upper\"),\\\n",
    "        substring(\"Country\", 1,3).alias(\"substring\"),\\\n",
    "        split(\"Country\", \" \").alias(\"split\"),\n",
    "        trim(\"Country\").alias(\"trim\"),\\\n",
    "        regexp_replace(\"Country\", \"e\", \"EE\").alias(\"regexpl_replace\"),\\\n",
    "        length(\"Country\").alias(\"length\")\\\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaeb04e-6d8c-4a5e-80b3-00249e322538",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!spark --cluster dataproc-course --session lesson-4.2 \n",
    "from pyspark.sql.functions import abs, round, ceil, pow, sqrt\n",
    "\n",
    "# Функции для работы с числами\n",
    "\n",
    "df\\\n",
    "    .dropDuplicates([\"CostPerItem\"])\\\n",
    "    .limit(15)\\\n",
    "    .select(\\\n",
    "        \"CostPerItem\",\\\n",
    "        abs(\"CostPerItem\").alias(\"abs\"),\\\n",
    "        ceil(\"CostPerItem\").alias(\"ceil\"),\\\n",
    "        round(pow(\"CostPerItem\", 2), 3).alias(\"powWithRound\"),\\\n",
    "        sqrt(abs(\"CostPerItem\")).alias(\"sqrt\")\\\n",
    "    ).sort(\"CostPerItem\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9fe901-2140-4622-b3d0-42e7e571808e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!spark --cluster dataproc-course --session lesson-4.2 \n",
    "from pyspark.sql.functions import col, current_date, current_timestamp, dayofmonth, date_add, date_format, to_timestamp, trunc, month, year\n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "# Функции для работы с датой и временем\n",
    "df\\\n",
    "    .select(\\\n",
    "            col(\"TransactionTime\"),\\\n",
    "            current_date().alias(\"CurrentDate\"),\\\n",
    "            current_timestamp().alias(\"CurrentTimestamp\"),\\\n",
    "            trunc(current_timestamp(), \"month\").alias(\"Trunc\"),\\\n",
    "            to_timestamp(col(\"TransactionTime\"), \"E MMM dd HH:mm:ss z yyyy\").alias(\"TransformedTranscation\"),\\\n",
    "            date_format(date_add(current_timestamp(), 30), \"dd/MM/yyyy\").alias(\"DateAdd\")\\\n",
    "           )\\\n",
    "    .withColumn(\"year\", year(\"TransformedTranscation\"))\\\n",
    "    .withColumn(\"month\", month(\"TransformedTranscation\"))\\\n",
    "    .withColumn(\"day\", dayofmonth(\"TransformedTranscation\"))\\\n",
    "    .show(10, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c91b53-50de-4f8b-a9d0-e8913882112b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!spark --cluster dataproc-course --session lesson-4.2 \n",
    "from pyspark.sql.functions import array_contains, array_join, array_sort, col, size, split\n",
    "\n",
    "# Функции для работы с массивами\n",
    "df\\\n",
    "    .select(col(\"Country\"), split(col(\"Country\"), \" \").alias(\"splittedCountry\"))\\\n",
    "    .distinct()\\\n",
    "    .sort(col(\"Country\").desc())\\\n",
    "    .select(col(\"Country\"),\\\n",
    "            col(\"splittedCountry\"),\\\n",
    "            array_contains(col(\"splittedCountry\"), \"United\").alias(\"contains\"),\\\n",
    "            size(col(\"splittedCountry\")).alias(\"size\"),\\\n",
    "            array_join(array_sort(col(\"splittedCountry\")), \"_\").alias(\"join\")\\\n",
    "           )\\\n",
    "    .show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1d610a-a48f-46da-b1a4-8242b295beaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
